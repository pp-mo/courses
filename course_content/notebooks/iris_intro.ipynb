{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to Iris\n",
    "\n",
    "## Course Aim\n",
    "\n",
    "The aim of this course is to introduce Iris and its main features and functionality, with a focus on how these relate to data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Iris cube\n",
    "\n",
    "**Learning outcome**: by the end of this section, you will be able to explain the capabilities and functionality of Iris cubes and coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top level object in Iris is called a cube. A cube contains data and metadata about a single phenomenon and is an implementation of the data model interpreted from the *Climate and Forecast (CF) Metadata Conventions*.\n",
    "\n",
    "Each cube has:\n",
    "\n",
    " * A data array (typically a NumPy array).\n",
    " * A \"name\", preferably a CF \"standard name\" to describe the phenomenon that the cube represents.\n",
    " * A collection of coordinates to describe each of the dimensions of the data array. These coordinates are split into two types:\n",
    "    * Dimensioned coordinates are numeric, monotonic and represent a single dimension of the data array. There may be only one dimensioned coordinate per data dimension.\n",
    "    * Auxilliary coordinates can be of any type, including discrete values such as strings, and may represent more than one data dimension.\n",
    "\n",
    "A fuller explanation is available in the [Iris user guide](http://scitools.org.uk/iris/docs/latest/userguide/iris_cubes.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a simple example to demonstrate the cube concept.\n",
    "\n",
    "Suppose we have a ``(3, 2, 4)`` NumPy array:\n",
    "\n",
    "![](../images/multi_array.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where dimensions 0, 1, and 2 have lengths 3, 2 and 4 respectively.\n",
    "\n",
    "The Iris cube to represent this data may consist of:\n",
    "\n",
    " * a standard name of \"air_temperature\" and units of \"kelvin\"\n",
    "\n",
    " * a data array of shape ``(3, 2, 4)``\n",
    "\n",
    " * a coordinate, mapping to dimension 0, consisting of:\n",
    "     * a standard name of \"height\" and units of \"meters\"\n",
    "     * an array of length 3 representing the 3 height points\n",
    "     \n",
    " * a coordinate, mapping to dimension 1, consisting of:\n",
    "     * a standard name of \"latitude\" and units of \"degrees\"\n",
    "     * an array of length 2 representing the 2 latitude points\n",
    "     * a coordinate system such that the latitude points could be fully located on the globe\n",
    "     \n",
    " * a coordinate, mapping to dimension 2, consisting of:\n",
    "     * a standard name of \"longitude\" and units of \"degrees\"\n",
    "     * an array of length 4 representing the 4 longitude points\n",
    "     * a coordinate system such that the longitude points could be fully located on the globe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pictorially the cube has taken on more information than a simple array:\n",
    "\n",
    "![](../images/multi_array_to_cube.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with a cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst it is possible to construct a cube by hand, a far more common approach to getting hold of a cube is to use the Iris load function to access data that already exists in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "cubes = iris.load(fname)\n",
    "print(cubes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we've loaded two cubes, one representing the \"surface_altitude\" and the other representing \"air_potential_temperature\". We can infer even more detail from this printout; for example, what are the dimensions and shape of the \"air_potential_temperature\" cube?\n",
    "\n",
    "Above we've printed the ``iris.cube.CubeList`` instance representing all of the cubes found in the given filename. However, we can see more detail by printing individual cubes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_pot_temp = cubes[0]\n",
    "print(air_pot_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cube attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = iris.load_cube(iris.sample_data_path('A1B_north_america.nc'))\n",
    "print(cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access a cube's data array the ``data`` property exists. This is either a NumPy array or in some cases a NumPy masked array. It is very important to note that for most of the supported filetypes in Iris, the cube's data isn't actually loaded until you request it via this property (either directly or indirectly). After you've accessed the data once, it is stored on the cube and thus won't be loaded from disk again.\n",
    "\n",
    "To find the shape of a cube's data it is possible to call ``cube.data.shape`` or ``cube.data.ndim``, but this will trigger any unloaded data to be loaded. Therefore ``shape`` and ``ndim`` are properties available directly on the cube that do not unnecessarily load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cube.shape)\n",
    "print(cube.ndim)\n",
    "print(type(cube.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``standard_name``, ``long_name`` and to an extent ``var_name`` are all attributes to describe the phenomenon that the cube represents. The ``name()`` method is a convenience that looks at the name attributes in the order they are listed above, returning the first non-empty string. To rename a cube, it is possible to set the attributes manually, but it is generally easier to use the ``rename()`` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cube.standard_name)\n",
    "print(cube.long_name)\n",
    "print(cube.var_name)\n",
    "print(cube.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.rename(\"A name that isn't a valid CF standard name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cube.standard_name)\n",
    "print(cube.long_name)\n",
    "print(cube.var_name)\n",
    "print(cube.name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``units`` attribute on a cube tells us the units of the numbers held in the data array. We can manually change the units, or better, we can convert the cube to another unit using the ``convert_units`` method, which will automatically update the data array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cube.units)\n",
    "print(cube.data.max())\n",
    "cube.convert_units('Celsius')\n",
    "print(cube.units)\n",
    "print(cube.data.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cube has a dictionary for extra general purpose attributes, which can be accessed with the ``cube.attributes`` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cube.attributes)\n",
    "print(cube.attributes['STASH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinates\n",
    "\n",
    "As we've seen, cubes need coordinate information to help us describe the underlying phenomenon. Typically a cube's coordinates are accessed with the ``coords`` or ``coord`` methods. The latter *must* return exactly one coordinate for the given parameter filters, where the former returns a list of matching coordinates, possibly of length 0.\n",
    "\n",
    "For example, to access the time coordinate, and print the first 4 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = cube.coord('time')\n",
    "print(time[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coordinate interface is very similar to that of a cube. The attributes that exist on both cubes and coordinates are: ``standard_name``, ``long_name``, ``var_name``, ``units``, ``attributes`` and ``shape``. Similarly, the ``name()``, ``rename()`` and ``convert_units()`` methods also exist on a coordinate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A coordinate does not have ``data``, instead it has ``points`` and ``bounds`` (``bounds`` may be ``None``). In Iris, time coordinates are currently represented as \"a number since an epoch\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repr(time.units))\n",
    "print(time.points[:4])\n",
    "print(time.bounds[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers can be converted to datetime objects with the unit's ``num2date`` method. Dates can be converted back again with the ``date2num`` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "print(time.units.num2date(time.points[:4]))\n",
    "print(time.units.date2num(datetime.datetime(1970, 2, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important attribute on a coordinate is its coordinate system. Coordinate systems may be ``None`` for trivial coordinates, but particularly for spatial coordinates, they may be complex definitions of things such as the projection, ellipse and/or datum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = cube.coord('latitude')\n",
    "print(lat.coord_system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the latitude's coordinate system is a simple geographic latitude on a spherical globe of radius 6371229 (meters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: the cube object\n",
    "\n",
    "* What advantages over NumPy arrays do Iris cubes provide?\n",
    "* What information does an Iris cube contain?\n",
    "* How do Iris cubes and coordinates relate to each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and saving data\n",
    "\n",
    "**Learning outcome**: by the end of this section, you will be able to use Iris to load datasets from disk as Iris cubes and save Iris cubes back to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris load functions\n",
    "\n",
    "There are three main load functions in Iris: ``load``, ``load_cube`` and ``load_cubes``.\n",
    "\n",
    "1. **load** is a general purpose loading function. Typically this is where all data analysis will start, before more loading is refined with the more controlled loading from the other two functions.\n",
    "2. **load_cube** returns a single cube from the given source(s) and constraint. There will be exactly one cube, or an exception will be raised.\n",
    "3. **load_cubes** returns a list of cubes from the given sources(s) and constraint(s). There will be exactly one cube per constraint, or an exception will be raised.\n",
    "\n",
    "\n",
    "Note: ``load_cube`` is a special case of ``load``, which can be seen with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('air_temp.pp')\n",
    "c1, = iris.load(fname)\n",
    "c2 = iris.load_cube(fname)\n",
    "c1 == c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving cubes\n",
    "\n",
    "The ``iris.save`` function provides a convenient interface to save Cube and CubeList instances.\n",
    "\n",
    "To save some cubes to a NetCDF file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "cubes = iris.load(fname)\n",
    "iris.save(cubes, 'saved_cubes.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ncdump -h saved_cubes.nc | head -n 20\n",
    "!rm saved_cubes.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra keywords can be passed to specific fileformat savers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-core Processing\n",
    "\n",
    "[Out-of-core processing](https://en.wikipedia.org/wiki/External_memory_algorithm) is a technical term that describes being able to process datasets that are too large to fit in memory at once. In Iris, this functionality is referred to as **lazy data**. It means that you can use Iris to load, process and save datasets that are too large to fit in memory without running out of memory. This is achieved by loading only the dataset's metadata and not the data array, unless this is specifically requested.\n",
    "\n",
    "To determine whether your cube has lazy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('air_temp.pp')\n",
    "cube = iris.load_cube(fname)\n",
    "print(cube.has_lazy_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris tries to maintain lazy data as much as possible. We refer to the operation of loading a cube's lazy data as 'realising' the cube's data. A cube's lazy data will only be loaded in a limited number of cases, including:\n",
    "\n",
    "* when the user directly requests the cube's data using `cube.data`,\n",
    "* when there is no lazy data processing algorithm available to perform the requested data processing, such as for peak finding, and\n",
    "* where actual data values are necessary, such as for cube plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.data\n",
    "print(cube.has_lazy_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "1\\. Load the file in ``iris.sample_data_path('atlantic_profiles.nc')`` and print the cube list. Store these cubes in a variable called `cubes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Loop through each of the cubes (e.g. ``for cube in cubes``) and print the standard name of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Index `cubes` to retrieve the `sea_water_potential_temperature` cube. Does this cube have lazy data? Print the minimum, maximum, mean and standard deviation of the cube's data. Does the cube still have lazy data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Get hold of the `latitude` coordinate on the `sea_water_potential_temperature` cube. Identify whether this coordinate has bounds. Print the minimum and maximum latitude points in the cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Go to the Iris reference documentation for ``iris.save``. What fileformats can Iris currently save to? What keywords are accepted to ``iris.save`` when saving a PP file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cube control and subsetting\n",
    "\n",
    "**Learning outcome**: by the end of this section, you will be able to apply Iris functionality to take a useful subset of an Iris cube and to combine multiple Iris cubes into a new larger cube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraints and Extract\n",
    "\n",
    "We've already seen the basic ``load`` function, but we can also control which cubes are actually loaded with *constraints*. The simplest constraint is just a string, which filters cubes based on their name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "print(iris.load(fname, 'air_potential_temperature'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris's constraints mechanism provides a powerful way to filter a subset of data from a larger collection. We've already seen that constraints can be used at load time to return data of interest from a file, but we can also apply constraints to a single cube, or a list of cubes, using their respective ``extract`` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubes = iris.load(fname)\n",
    "print(cubes.extract('air_potential_temperature'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest constraint, namely a string that matches a cube's name, is conveniently converted into an actual ``iris.Constraint`` instance wherever needed. However, we could construct this constraint manually and compare with the previous result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pot_temperature_constraint = iris.Constraint('air_potential_temperature')\n",
    "print(cubes.extract(pot_temperature_constraint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Constraint constructor also takes arbitrary keywords to constrain coordinate values. For example, to extract model level number 10 from the air potential temperature cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pot_temperature_constraint = iris.Constraint('air_potential_temperature',\n",
    "                                             model_level_number=10)\n",
    "print(cubes.extract(pot_temperature_constraint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass a list of possible values, and even combine two constraints with ``&``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cubes.extract('air_potential_temperature' & \n",
    "                    iris.Constraint(model_level_number=[4, 10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define arbitrary functions that operate on each cell of a coordinate. This is a common thing to do for floating point coordinates, where exact equality is non-trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def less_than_10(cell):\n",
    "    \"\"\"Return True for values that are less than 10.\"\"\"\n",
    "    return cell < 10\n",
    "\n",
    "print(cubes.extract(iris.Constraint('air_potential_temperature',\n",
    "                                    model_level_number=less_than_10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Constraints\n",
    "\n",
    "It is common to want to build a constraint for time. This can be achieved by comparing cells containing datetimes, which is available by default in Iris v2. If you are using Iris v1, please uncomment and run the code cell below to enable this functionality globally within the session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris.FUTURE.cell_datetime_objects = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few different approaches for producing time constraints in Iris. We will focus here on one approach for constraining on time in Iris. \n",
    "\n",
    "This approach allows us to access individual components of cell datetime objects and run comparisons on those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_constraint = iris.Constraint(time=lambda cell: cell.point.hour == 11)\n",
    "print(air_pot_temp.extract(time_constraint).summary(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Cell methods are a part of cube metadata that record statistical operations that have been applied to a cube. For example, \"`mean: time (6hrs)`\" tells us that the cube has had a time mean over a 6hr interval applied.\n",
    "\n",
    "We can determine what, if any, cell methods a cube has with the attribute `cube.cell_methods`. The following function, then, tells us whether or not a cube has cell methods:\n",
    "\n",
    "```python\n",
    "def has_cell_methods(cube):\n",
    "    return len(cube.cell_methods) > 0\n",
    "```\n",
    "\n",
    "1\\. With the cubes loaded from ``[iris.sample_data_path('A1B_north_america.nc'), iris.sample_data_path('uk_hires.pp')]`` use the CubeList's **``extract``** method to filter only the cubes that have cell methods. (Hint: Look at the ``iris.Constraint`` documentation for the **cube_func** keyword). You should find that the 3 cubes are whittled down to just 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Using the file found at ``iris.sample_data_path('A1B_north_america.nc')`` filter the cube, using constraints, such that only data between 1860 and 1980 remains (hint: This data has a 360-day calendar with yearly data from 1860 to 2100, so we will need to access the individual components of the cell point's datetime, to return a time dimension of length 120)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "Cubes can be indexed in a familiar manner to that of NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "cube = iris.load_cube(fname, 'air_potential_temperature')\n",
    "print(cube.summary(shorten=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcube = cube[..., ::2, 15:35, :10]\n",
    "subcube.summary(shorten=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the result of indexing a cube is *always* a copy and never a *view* on the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration\n",
    "\n",
    "We can loop through all desired subcubes in a larger cube using the cube methods ``slices`` and ``slices_over``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "cube = iris.load_cube(fname,\n",
    "                      iris.Constraint('air_potential_temperature',\n",
    "                                      model_level_number=1))\n",
    "print(cube.summary(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **``slices``** method returns all the slices of a cube on the dimensions specified by the coordinates passed to the slices method.\n",
    "\n",
    "So in this example, each `grid_latitude` / `grid_longitude` slice of the cube is returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subcube in cube.slices(['grid_latitude', 'grid_longitude']):\n",
    "    print(subcube.summary(shorten=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use **``slices_over``** to return one subcube for each coordinate value in a specified coordinate. This helps us when trying to retrieve all the slices along a given cube dimension.\n",
    "\n",
    "For example, let's consider retrieving all the slices over the time dimension (i.e. each time step in its own cube with a scalar time coordinate) using ``slices``. As per the above example, to achieve this using ``slices`` we would have to specify all the cube's dimensions _except_ the time dimension.\n",
    "\n",
    "Let's take a look at ``slices_over`` providing this functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "cube = iris.load_cube(fname, 'air_potential_temperature')\n",
    "for subcube in cube.slices_over('model_level_number'):\n",
    "    print(subcube.summary(shorten=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: Indexing and slicing\n",
    "\n",
    "* What are the similarities between indexing and slicing?\n",
    "* What are the differences?\n",
    "* Which cube slicing method would be easiest to use to return all subcubes along the realization dimension?\n",
    "* Which cube slicing method would be easiest to use to return all horizontal 2D slices in a 4D cube?\n",
    "* In what situations would indexing be the best way to subset a cube? What about slicing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge\n",
    "\n",
    "When Iris loads data it tries to reduce the number of cubes returned by collecting together multiple fields with\n",
    "shared metadata into a single multidimensional cube. In Iris, this is known as merging.\n",
    "\n",
    "In order to merge two cubes, they must be identical in everything but a scalar dimension, which goes on to become a new data dimension.\n",
    "\n",
    "The ``iris.load_raw`` function can be used as a diagnostic tool to identify the individual \"fields\" that Iris identifies in a given set of filenames before any merge takes place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('GloSea4', 'ensemble_008.pp')\n",
    "raw_cubes = iris.load_raw(fname)\n",
    "\n",
    "print(len(raw_cubes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look in detail at these cubes, we find that they are identical in every coordinate except for the scalar forecast_period and time coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_cubes[0])\n",
    "print('--' * 50)\n",
    "print(raw_cubes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any CubeList can be merged with the ``merge`` method, and the resulting CubeList from load_raw is no different.\n",
    "The ``merge`` method *always* returns another CubeList:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_cube, = raw_cubes.merge()\n",
    "print(merged_cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look in more detail, we can see that the time coordinate has become a new dimension, as well as gaining another forecast_period auxiliary coordinate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_cube.coord('time'))\n",
    "print(merged_cube.coord('forecast_period'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying merge problems\n",
    "\n",
    "In order to avoid the Iris merge functionality making often inappropriate assumptions about incoming data, merge is strict with regards to the uniformity of the incoming cubes.\n",
    "\n",
    "For example, if we load the fields from two ensemble members from the GloSea4 model sample data, we see we have 12 fields before any merge takes place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('GloSea4', 'ensemble_00[34].pp')\n",
    "cubes = iris.load_raw(fname, 'surface_temperature')\n",
    "print(len(cubes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to merge these 12 cubes we get 2 cubes rather than one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_cubes = cubes.merge(unique=False)\n",
    "print(incomplete_cubes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look in more detail at these two cubes, what is different between the two? (Hint: One value changes, another is completely missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(incomplete_cubes[0])\n",
    "print('--' * 50)\n",
    "print(incomplete_cubes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding the missing coordinate, we can trigger a merge of the 12 cubes into a single cube, as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cube in cubes:\n",
    "    if not cube.coords('realization'):\n",
    "        cube.add_aux_coord(iris.coords.DimCoord(np.int32(3),\n",
    "                                                'realization'))\n",
    "\n",
    "merged_cubes = cubes.merge()\n",
    "print(merged_cubes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris includes functionality to simplify the identification process for causes of failed merges. The ``merge_cube`` method of a CubeList expects the list of cubes to contain only cubes that can be merged to produce a single cube. If they do not merge to a single cube, a descriptive exception will be raised. For instance:\n",
    "\n",
    "```\n",
    "   >>> cubes.merge_cube()\n",
    "   Traceback (most recent call last):\n",
    "     File \"<stdin>\", line 1, in <module>\n",
    "     ...\n",
    "   iris.exceptions.MergeError: failed to merge into a single cube.\n",
    "     Coordinates in cube.aux_coords (scalar) differ: realization.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate ###\n",
    "\n",
    "We have seen that merge combines a list of cubes with a common scalar coordinate to produce a single cube with a new dimension created from these scalar values.\n",
    "\n",
    "But what happens if you try to combine cubes along a common dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('A1B_north_america.nc')\n",
    "cube = iris.load_cube(fname)\n",
    "\n",
    "cube_1 = cube[:10]\n",
    "cube_2 = cube[10:20]\n",
    "cubes = iris.cube.CubeList([cube_1, cube_2])\n",
    "print(cubes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These cubes should be able to be merged; after all, they have both come from the same original cube!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cubes.merge())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge cannot be used to combine common non-scalar coordinates. Instead we must use concatenate, which joins together (\"concatenates\") common non-scalar coordinates to produce a single cube with the common dimension extended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cubes.concatenate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with merge, Iris contains functionality to simplify the identification process for causes of failed concatenations. The ``concatenate_cube`` method of a CubeList expects the list of cubes to contain only cubes that can be concatenated to produce a single cube. If they do not concatenate to a single cube, a descriptive error will be raised. For instance:\n",
    "\n",
    "```\n",
    "    >>> print cubes.concatenate_cube()\n",
    "    Traceback (most recent call last):\n",
    "      ...\n",
    "    iris.exceptions.ConcatenateError: failed to concatenate into a single cube.\n",
    "      Scalar coordinates differ: forecast_reference_time, height != forecast_reference_time\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Exercise 3\n",
    "\n",
    "The following exercise is designed to give you experience of solving issues that prevent a merge from taking place.\n",
    "The output from ``merge_cube`` is included to help with identification, and once a fix has been identified, ``raw_cubes.merge()`` should result in a CubeList containing a single cube:\n",
    "\n",
    "The first exercise is completed below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Identify and resolve the issue preventing the merge of ``air_potential_temperature`` cubes from ``resources/merge_exercise.1.*.nc``.\n",
    "\n",
    "    >>> raw_cubes = iris.load_raw('../resources/merge_exercise.1.*.nc', 'air_potential_temperature')\n",
    "    >>> raw_cubes.merge_cube()\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    iris.exceptions.MergeError: failed to merge into a single cube.\n",
    "      cube.attributes keys differ: 'History'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cubes = iris.load_raw('../resources/merge_exercise.1.*.nc', 'air_potential_temperature')\n",
    "\n",
    "# Print the attributes, clearly one is different.\n",
    "for cube in raw_cubes:\n",
    "    print(cube.attributes)\n",
    "\n",
    "# Remove the history attribute from the first cube.\n",
    "del raw_cubes[0].attributes['History']\n",
    "\n",
    "# Check that this has meant that a merge now results in a single cube.\n",
    "print('--' * 50)\n",
    "print(raw_cubes.merge())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Identify and resolve the issue preventing the merge of ``air_potential_temperature`` cubes from ``resources/merge_exercise.5.*.nc`` (hint: can these cubes be merged?).\n",
    "\n",
    "    >>> raw_cubes = iris.load_raw('../resources/merge_exercise.5.*.nc', 'air_potential_temperature')\n",
    "    >>> raw_cubes.merge_cube()\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    iris.exceptions.MergeError: failed to merge into a single cube.\n",
    "      Coordinates in cube.dim_coords differ: time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "**Learning outcome**: by the end of this section, you will be able to use Iris to visualise weather and climate datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "\n",
    "Iris comes with two plotting modules called ``iris.plot`` and ``iris.quickplot`` that wrap some of the common matplotlib plotting functions such that cubes can be passed as input rather than the usual NumPy arrays. The two modules are very similar, with the primary difference being that ``quickplot`` will add extra information to the axes, such as:\n",
    "\n",
    " * a colorbar,\n",
    " * labels for the x and y axes, and\n",
    " * a title where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris.plot as iplt\n",
    "import iris.quickplot as qplt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = iris.load_cube(iris.sample_data_path('A1B_north_america.nc'))\n",
    "ts = cube[-1, 20, ...]\n",
    "print(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplt.plot(ts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, lets plot the result of ``iplt.plot`` next to ``qplt.plot``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "iplt.plot(ts)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "qplt.plot(ts)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the result of qplt has axis labels and a title; everything else about the axes is identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plotting functions in Iris have strict rules on the dimensionality of the inputted cubes. For example, a 2d cube is needed in order to create a contour plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qplt.contourf(cube[:, 0, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maps with cartopy\n",
    "\n",
    "When the result of a plot operation is a map, Iris will automatically create an appropriate cartopy axes if one doesn't already exist.\n",
    "\n",
    "We can use matplotlib's `gca()` function to get hold of the automatically created cartopy axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "qplt.contourf(cube[0, ...], 25)\n",
    "ax = plt.gca()\n",
    "ax.coastlines()\n",
    "\n",
    "ax = plt.subplot(1, 2, 2, projection=ccrs.RotatedPole(100, 37))\n",
    "qplt.contourf(cube[0, ...], 25)\n",
    "ax.coastlines()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Use the above cube, with appropriate indexing, to produce the following:\n",
    "\n",
    "1\\. a **contourf** map on a LambertConformal projection (with coastlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. a block plot (**pcolormesh**) map in its native projection  (with coastlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. a **scatter** plot showing *air_temperature* vs *longitude* (hint: the inputs to scatter can be a combination of coordinates or 1D cubes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "**Learning outcome**: by the end of this section, you will be able to use Iris to apply mathematical operations on cube data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cube maths\n",
    "\n",
    "Basic mathematical operators exist on the cube to allow one to add, subtract, divide, multiply and perform other mathematical operations on cubes of a similar shape to one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1b = iris.load_cube(iris.sample_data_path('A1B_north_america.nc'))\n",
    "e1 = iris.load_cube(iris.sample_data_path('E1_north_america.nc'))\n",
    "\n",
    "print(e1.summary(True))\n",
    "print(a1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_difference = a1b - e1\n",
    "print(scenario_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the resultant cube's name is now `unknown` and that resultant cube's `attributes` and `cell_methods` have disappeared; this is because these all differed between the two input cubes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to operate on cubes with numeric scalars, NumPy arrays and even cube coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(e1 * e1.coord('latitude'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cube broadcasting is also taking place, meaning that the two inputs (cube, coordinate, array, or even constant value) don't need to have the same shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(e1 + 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've just seen, we have the ability to update the cube's data directly. Whenever we do this though, we should be mindful of updating appropriate metadata on the cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1_hot = e1.copy()\n",
    "\n",
    "e1_hot.data = np.ma.masked_less_equal(e1_hot.data, 280)\n",
    "e1_hot.rename('air temperatures greater than 280K')\n",
    "print(e1_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cube aggregation and statistics\n",
    "\n",
    "Many standard univariate aggregations exist in Iris. Aggregations allow one or more dimensions of a cube to be statistically collapsed for the purposes of statistical analysis of the cube's data. Iris uses the term 'aggregators' to refer to the statistical operations that can be used for aggregation.\n",
    "\n",
    "A list of aggregators is available at http://scitools.org.uk/iris/docs/latest/iris/iris/analysis.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "cube = iris.load_cube(fname, 'air_potential_temperature')\n",
    "print(cube.summary(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take the vertical mean of this cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cube.collapsed('model_level_number', iris.analysis.MEAN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: Exercise 5\n",
    "\n",
    "Let's apply all that we've learned about data processing and visualisation in Iris. We will perform data processing and visualisation to compare two possible climate futures scenarios, called the A1B scenario and the E1 scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Load as cubes the datasets found at `iris.sample_data_path('E1_north_america.nc')` and `iris.sample_data_path('A1B_north_america.nc')`. Print the summary of each cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a\\. Plot the following data in a single figure with three maps in one row:\n",
    "\n",
    " * the air temperature in the E1 scenario in 2099, \n",
    " * the air temperature in the A1B scenario in 2099, and\n",
    " * the difference between the two scenarios.\n",
    "\n",
    "Think about the most appropriate matplotlib colormap(s) to use for each plot. Hint: the different matplotlib colormaps can be seen at https://matplotlib.org/1.5.3/examples/color/colormaps_reference.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b\\. What information do your plots show? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Produce cubes that describe the area-averaged air temperature over time for each scenario. Calculate the model difference between these two cubes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Produce a single line plot with the data from the two cubes you produced in part 3. Make sure you label the lines you plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Extract and compare the area-averaged air temperature values for both scenarios in 1980 and 2099. What conclusions can you draw from the values you've extracted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced concepts\n",
    "\n",
    "None of the concepts covered in this section are essential for using Iris, but are useful to know. We'll cover them if time permits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load callbacks\n",
    "\n",
    "Sometimes important data exists in a filename rather than in the file itself, and it is desirable for it to become part of the cube's metadata.\n",
    "For example, some early GloSea4 model runs recorded the \"ensemble member number\" (or \"realization\" in CF terms) in the filename, but not in actual PP metadata itself. As a result, loading the data yielded 2 cubes, rather than a single, fully merged, cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('GloSea4', 'ensemble_00[34].pp')\n",
    "for cube in iris.load(fname, 'surface_temperature'):\n",
    "    print(cube, '\\n', '--' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resolve this we can define a function that gets called during the load process. This must take as arguments:\n",
    "\n",
    " * a cube,\n",
    " * a 2D field - either a PP field, a NetCDF variable or a GRIB message depending on the file format being loaded, and\n",
    " * a filename.\n",
    "\n",
    "In this case, the function makes the necessary adjustments to include a \"realization\" coordinate. We pass this function to load, and the result is a successfully merged cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def realization_callback(cube, field, fname):\n",
    "    basename = os.path.basename(fname)\n",
    "    if not cube.coords('realization') and basename.startswith('ensemble_'):\n",
    "        cube.add_aux_coord(iris.coords.DimCoord(np.int32(basename[-6:-3]),\n",
    "                                                'realization'))\n",
    "\n",
    "print(iris.load_cube(fname, callback=realization_callback))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating extra annotation coordinates for statistical convenience\n",
    "\n",
    "Sometimes we want to be able to categorise data before performing statistical operations on it. For example, we might want to categorise our data by \"daylight maximum\" or \"seasonal mean\" etc. Both of these categorisations would be based on the time coordinate.\n",
    "\n",
    "The ``iris.coord_categorisation`` module provides convenience functions to add some common categorical coordinates, and provides a generalised function to allow each creation of custom categorisations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris.coord_categorisation as coord_cat\n",
    "\n",
    "filename = iris.sample_data_path('ostia_monthly.nc')\n",
    "cube = iris.load_cube(filename, 'surface_temperature')\n",
    "print(cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cube loaded represents the monthly air_temperature from April 2006 through to October 2010. Let's add a categorisation coordinate to this cube to identify the climatological season (i.e \"djf\", \"mam\", \"jja\" or \"son\") of each time point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_cat.add_season(cube, 'time', name='clim_season')\n",
    "print(cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cube.coord('clim_season'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the cube's ``aggregated_by`` method to \"group by and aggregate\" on the season, to produce the seasonal mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_mean = cube.aggregated_by('clim_season', iris.analysis.MEAN)\n",
    "print(seasonal_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take this further by extracting by our newly created coordinate, producing a plot of the winter zonal mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winter = seasonal_mean.extract(iris.Constraint(clim_season='djf'))\n",
    "\n",
    "qplt.plot(winter.collapsed('latitude', iris.analysis.MEAN))\n",
    "plt.title('Winter zonal mean surface temperature at $\\pm5^{\\circ}$ latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom categorisation\n",
    "\n",
    "Custom categorisation can be achieved with an arbitrary function. For example, the existing ``add_year`` categorisor takes the 'time' coordinate, and creates a 'year' coordinate. This could be achieved without using the available ``add_year`` by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_from_time(coord, point):\n",
    "    return coord.units.num2date(point).year\n",
    "\n",
    "coord_cat.add_categorised_coord(cube, 'year', cube.coord('time'),\n",
    "                                year_from_time)\n",
    "\n",
    "print(cube.coord('year'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance tricks\n",
    "\n",
    "This section details a few common tricks to improve the performance of your Iris code:\n",
    "\n",
    " * Data loading.\n",
    " * Load once, extract many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make use of deferred loading of data\n",
    "\n",
    "Sometimes it makes sense to load data before doing operations, other times it makes sense to do data reduction before loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zonal_sum(cube):\n",
    "    \"\"\"\n",
    "    A really silly function to calculate the sum of the grid_longitude\n",
    "    dimension.\n",
    "    Don't use this in real life, instead consider doing:\n",
    "    \n",
    "        cube.collapsed('grid_longitude', iris.analysis.SUM)\n",
    "    \n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for i, _ in enumerate(cube.coord('grid_longitude')):\n",
    "        total += cube[..., i].data\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "pt = iris.load_cube(fname, 'air_potential_temperature')\n",
    "result = zonal_sum(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact same code, only with the data loaded upfront:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "pt = iris.load_cube(fname, 'air_potential_temperature')\n",
    "pt.data\n",
    "result = zonal_sum(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load once, extract many times\n",
    "\n",
    "Iris loading can be slow, particularly if the format stores 2d fields of a conceptually higher dimensional dataset, as is the case with GRIB and PP. To maximise load speed and avoid unncecessary processing, it is worth constraining the fields that are of interest *at load time*, but there is no caching, so loading a file twice will be twice as slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = iris.sample_data_path('uk_hires.pp')\n",
    "model_levels = [1, 4, 7, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "for model_level in model_levels:\n",
    "    pt = iris.load_cube(fname,\n",
    "                        iris.Constraint('air_potential_temperature',\n",
    "                                        model_level_number=model_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "cubes = iris.load(fname)\n",
    "for model_level in model_levels:\n",
    "    pt = cubes.extract(iris.Constraint('air_potential_temperature',\n",
    "                                       model_level_number=model_level),\n",
    "                       strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For files with lots of different phenomenon this can be improved further by loading only the phenomenon (and in this case just the model levels of interest):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "cube = iris.load(fname,\n",
    "                 iris.Constraint('air_potential_temperature',\n",
    "                                 model_level_number=model_levels))\n",
    "for model_level in model_levels:\n",
    "    pt = cube.extract(iris.Constraint(model_level_number=model_level))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Exercise\n",
    "\n",
    "Produce a set of plots that provide a comparison of decadal-mean air temperatures over North America:\n",
    "\n",
    "**Part 1**\n",
    "\n",
    "Load 'A1B_north_america.nc' from the Iris sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2**\n",
    "\n",
    "Extract just data from the year 1980 and beyond from the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3**\n",
    "\n",
    "Define a function that takes a coordinate and a single time point as arguments, and returns the decade. For example, your function should return 2010 for the following:\n",
    "\n",
    "```python\n",
    "time = iris.coords.DimCoord([10], 'time',\n",
    "                           units='days since 2018-01-01')\n",
    "print(your_decade_function(time, time.points[0]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4**\n",
    "\n",
    "Add a \"decade\" coordinate to the loaded cube using your function and the coord categorisation module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 5**\n",
    "\n",
    "Calculate the decadal means cube for this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 6**\n",
    "\n",
    "Create a figure with 3 rows and 4 columns displaying the decadal means, with the decade displayed prominently in each axes' title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
